# Discord Anti-Spam Bot: Database Implementation

## Overview

This document outlines the database implementation plan for the Discord Anti-Spam Bot, transforming it from an in-memory system to a fully stateful application using Supabase as the database backend. This enables configuration storage, user tracking, analytics collection, and training data gathering for future model improvements.

## Technology Choice: Supabase

Supabase was selected as our database solution for several reasons:

1. **Postgres Backend**: Industry-standard relational database with robust feature set
2. **Built-in Row-Level Security**: Granular security controls for multi-tenant applications
3. **Realtime Capabilities**: Support for realtime subscriptions to database changes
4. **Edge Functions**: Ability to run serverless functions close to the database
5. **Simplified API**: RESTful and GraphQL interfaces reduce development time
6. **Vector Support**: Native pgvector extension for potential AI feature expansion
7. **TypeScript Integration**: Strong TypeScript support for type safety

## Goals

1. **Configuration Storage:** Move bot configuration from environment variables to database
2. **User Tracking:** Persist suspicious user data across bot restarts
3. **Analytics Collection:** Gather metrics to demonstrate bot effectiveness
4. **Training Data:** Collect and store data for future model training
5. **Cross-Server Intelligence:** Enable information sharing across multiple Discord servers

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚   Discord Bot   â”‚â—„â”€â”€â”€â”¤  Service Layer  â”‚â—„â”€â”€â”€â”¤ Data Repository â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚               â”‚
                                              â”‚   Supabase    â”‚
                                              â”‚               â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Database Schema

### Current Tables

The following tables are currently implemented in the schema:

```sql
-- Servers table (guild configuration)
CREATE TABLE IF NOT EXISTS servers (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  guild_id TEXT UNIQUE NOT NULL,
  restricted_role_id TEXT,
  admin_channel_id TEXT,
  verification_channel_id TEXT,
  admin_notification_role_id TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  settings JSONB DEFAULT '{}'::JSONB,
  is_active BOOLEAN DEFAULT TRUE
);

COMMENT ON TABLE servers IS 'Discord servers where the bot is installed';
```

### Planned Tables

The following tables are defined in the schema but not yet fully utilized:

```sql
-- Users table
CREATE TABLE IF NOT EXISTS users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  discord_id TEXT NOT NULL,
  username TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  global_reputation_score REAL,
  account_created_at TIMESTAMP WITH TIME ZONE,
  metadata JSONB DEFAULT '{}'::JSONB,
  UNIQUE(discord_id)
);

COMMENT ON TABLE users IS 'Discord users across all servers';
COMMENT ON COLUMN users.global_reputation_score IS 'Cross-server reputation score (higher is more trusted)';
```

```sql
-- Server members (users in specific servers)
CREATE TABLE IF NOT EXISTS server_members (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  join_date TIMESTAMP WITH TIME ZONE,
  reputation_score REAL DEFAULT 0.0,
  is_restricted BOOLEAN DEFAULT FALSE,
  last_verified_at TIMESTAMP WITH TIME ZONE,
  last_message_at TIMESTAMP WITH TIME ZONE,
  message_count INTEGER DEFAULT 0,
  UNIQUE(server_id, user_id)
);

CREATE INDEX idx_server_members_server ON server_members(server_id);
CREATE INDEX idx_server_members_user ON server_members(user_id);

COMMENT ON TABLE server_members IS 'Mapping table for users in specific Discord servers';
```

The following tables are planned for future implementation:

```sql
-- Detection events
CREATE TABLE IF NOT EXISTS detection_events (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  message_id TEXT,
  detection_type TEXT NOT NULL,
  confidence REAL,
  confidence_level TEXT, -- 'Low', 'Medium', 'High'
  reasons TEXT[],
  used_gpt BOOLEAN DEFAULT FALSE,
  detected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  admin_action TEXT, -- 'Verified', 'Banned', 'Ignored'
  admin_action_by TEXT, -- Discord ID of admin who took action
  admin_action_at TIMESTAMP WITH TIME ZONE,
  metadata JSONB DEFAULT '{}'::JSONB
);

CREATE INDEX idx_detection_events_server ON detection_events(server_id);
CREATE INDEX idx_detection_events_user ON detection_events(user_id);
CREATE INDEX idx_detection_events_date ON detection_events(detected_at);

COMMENT ON TABLE detection_events IS 'Records of spam detection incidents';
```

```sql
-- Messages (for suspicious users)
CREATE TABLE IF NOT EXISTS messages (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  message_id TEXT NOT NULL,
  content TEXT,
  sent_at TIMESTAMP WITH TIME ZONE,
  channel_id TEXT,
  is_flagged BOOLEAN DEFAULT FALSE,
  metadata JSONB DEFAULT '{}'::JSONB,
  UNIQUE(server_id, message_id)
);

CREATE INDEX idx_messages_server ON messages(server_id);
CREATE INDEX idx_messages_user ON messages(user_id);
CREATE INDEX idx_messages_date ON messages(sent_at);

COMMENT ON TABLE messages IS 'Message content for flagged users (limited retention)';
```

```sql
-- Verification threads
CREATE TABLE IF NOT EXISTS verification_threads (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  thread_id TEXT NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  closed_at TIMESTAMP WITH TIME ZONE,
  outcome TEXT, -- 'Verified', 'Banned', 'Timeout', 'Abandoned'
  admin_id TEXT,
  notes TEXT,
  metadata JSONB DEFAULT '{}'::JSONB,
  UNIQUE(server_id, thread_id)
);

CREATE INDEX idx_verification_threads_server ON verification_threads(server_id);
CREATE INDEX idx_verification_threads_user ON verification_threads(user_id);

COMMENT ON TABLE verification_threads IS 'Verification threads for suspicious users';
```

```sql
-- Analytics
CREATE TABLE IF NOT EXISTS analytics (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  date DATE NOT NULL,
  detection_count INTEGER DEFAULT 0,
  verification_count INTEGER DEFAULT 0,
  ban_count INTEGER DEFAULT 0,
  verify_count INTEGER DEFAULT 0,
  false_positive_count INTEGER DEFAULT 0,
  gpt_call_count INTEGER DEFAULT 0,
  message_count INTEGER DEFAULT 0,
  join_count INTEGER DEFAULT 0,
  metrics JSONB DEFAULT '{}'::JSONB,
  UNIQUE(server_id, date)
);

CREATE INDEX idx_analytics_server ON analytics(server_id);
CREATE INDEX idx_analytics_date ON analytics(date);

COMMENT ON TABLE analytics IS 'Daily aggregated metrics per server';
```

```sql
-- Server trust network
CREATE TABLE IF NOT EXISTS server_relationships (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  source_server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  target_server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  relationship_type TEXT NOT NULL, -- 'Trust', 'Distrust', 'Neutral'
  trust_level INTEGER DEFAULT 0,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  created_by TEXT, -- Discord ID of admin who created relationship
  UNIQUE(source_server_id, target_server_id)
);

CREATE INDEX idx_server_relationships_source ON server_relationships(source_server_id);
CREATE INDEX idx_server_relationships_target ON server_relationships(target_server_id);

COMMENT ON TABLE server_relationships IS 'Trust relationships between servers';
```

```sql
-- Training data
CREATE TABLE IF NOT EXISTS training_data (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id),
  content TEXT,
  context JSONB,
  label TEXT, -- 'SPAM', 'NOT_SPAM', 'UNCERTAIN'
  labeled_by TEXT,
  labeled_at TIMESTAMP WITH TIME ZONE,
  collected_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  quality_score INTEGER, -- 1-5, higher is better training data
  metadata JSONB DEFAULT '{}'::JSONB
);

CREATE INDEX idx_training_data_server ON training_data(server_id);
CREATE INDEX idx_training_data_label ON training_data(label);
CREATE INDEX idx_training_data_quality ON training_data(quality_score);

COMMENT ON TABLE training_data IS 'Collected data for future model training';
```

```sql
-- GPT usage tracking
CREATE TABLE gpt_usage (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  server_id UUID REFERENCES servers(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id),
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  model TEXT NOT NULL,
  prompt_tokens INTEGER NOT NULL,
  completion_tokens INTEGER NOT NULL,
  total_tokens INTEGER NOT NULL,
  purpose TEXT, -- 'UserClassification', 'MessageAnalysis', etc.
  success BOOLEAN DEFAULT TRUE,
  latency_ms INTEGER,
  metadata JSONB DEFAULT '{}'::JSONB
);

CREATE INDEX idx_gpt_usage_server ON gpt_usage(server_id);
CREATE INDEX idx_gpt_usage_date ON gpt_usage(timestamp);

COMMENT ON TABLE gpt_usage IS 'Tracks GPT API usage for cost analysis';
```

## Implementation Plan

The database implementation is divided into several chunks, each focusing on a specific aspect of the system:

### Chunk H1: Supabase Setup & Infrastructure

- âœ… Supabase project creation
  - âœ… Configure authentication settings
  - âœ… Document API keys and endpoints
  - âœ… Add keys to environment variables
- âœ… Database schema design
  - âœ… Document relationships and constraints
  - âœ… Create SQL migration scripts
  - âœ… Test schema with sample data
- âœ… Repository layer setup
  - âœ… Create base repository interface
  - âœ… Implement SupabaseClient connection management
  - âœ… Add error handling and retries
  - âœ… Create TypeScript types for database entities
- ğŸ”„ Testing infrastructure
  - â³ Set up test database in Supabase
  - âœ… Create mock repositories for testing (ServerRepository)
  - ğŸ”„ Implement integration tests for repositories (basic tests only)
  - â³ Create fixtures for test data
  - â³ Add test isolation strategies (unique IDs, cleanup hooks)
  - â³ Implement transaction-based test rollbacks

### Chunk H2: Core Entity Management

- âœ… Server configuration repository
  - âœ… Create servers table
  - âœ… Implement CRUD operations
  - âœ… Add caching for frequently accessed configuration
  - âœ… Create unit tests with proper mocking strategies
  - âœ… Add server initialization on bot startup
  - âœ… Handle new guild joins with guildCreate event
  - âœ… Add behavior-based tests for configuration flow
- ğŸ”„ User repository
  - âœ… Create users table schema with Discord metadata
  - ğŸ”„ Implement user lookup and creation
  - ğŸ”„ Add methods for user history
  - â³ Create unit tests with proper isolation
  - â³ Add integration tests for user workflows
- âœ… Configuration management service
  - âœ… Create service for managing server configurations
  - âœ… Implement fallback to defaults
  - âœ… Add validation logic
  - âœ… Create unit tests with proper abstraction levels
  - ğŸ”„ Add integration tests for config persistence
  - ğŸ”„ Document configuration flow and test cases
- ğŸ”„ User management service
  - ğŸ”„ Create service for user operations
  - ğŸ”„ Add methods for tracking user status
  - ğŸ”„ Implement user reputation calculation
  - ğŸ”„ Create unit tests with mock implementations
  - ğŸ”„ Add integration tests for user workflows

### Chunk H3: Detection History & Flagging

- ğŸ”„ Detection events repository
  - â³ Create detection_events table
  - ğŸ”„ Implement methods to record detection outcomes
  - ğŸ”„ Add querying capabilities
  - â³ Create unit tests with proper isolation
  - â³ Add performance tests for high-volume scenarios
- ğŸ”„ User flags repository
  - â³ Create user_flags table
  - ğŸ”„ Add methods for flag management
  - ğŸ”„ Implement flag history and status tracking
  - â³ Create unit tests with transaction rollbacks
  - â³ Add integration tests for flag workflows
- ğŸ”„ DetectionOrchestrator integration
  - ğŸ”„ Update orchestrator to use repositories
  - ğŸ”„ Store detection results
  - â³ Retrieve historical data for context
  - ğŸ”„ Create unit tests with proper mocking
  - ğŸ”„ Add integration tests for full detection flow
- ğŸ”„ Thread & verification tracking
  - â³ Create verification_threads table
  - ğŸ”„ Track verification outcomes
  - ğŸ”„ Store thread references
  - â³ Create unit tests with cleanup hooks
  - ğŸ”„ Add integration tests for verification flow

### Chunk H4: Message & Context Storage

- â³ Message repository
- â³ Context repository
- â³ GPTService integration
- â³ HeuristicService integration

### Chunk H5: Analytics & Insights

- â³ Analytics repository
- â³ Analytics service
- â³ Admin commands for analytics
- â³ Performance metrics

### Chunk H6: Environment Transition

- ğŸ”„ Config migration tool
  - ğŸ”„ Create tool to migrate env vars to database
  - â³ Support bulk imports
  - ğŸ”„ Add validation and logging
  - ğŸ”„ Create unit tests with proper isolation
  - âœ… Add integration tests for migration flows
- âœ… Configuration UI
  - âœ… Add Discord commands for configuration
  - âœ… Implement configuration verification
  - âœ… Add help documentation
  - âœ… Create unit tests with proper mocking
  - âœ… Add integration tests for UI flows
- â³ Backup & restore
- ğŸ”„ Environment detection
  - ğŸ”„ Add environment awareness (dev/test/prod)
  - ğŸ”„ Implement appropriate logging levels
  - ğŸ”„ Configure fallbacks for each environment
  - âœ… Create unit tests with proper isolation
  - âœ… Add integration tests for environment switching

### Chunk H7: Multi-Server Intelligence

- â³ Cross-server schema extensions
- â³ Trust network service
- â³ Shared intelligence
- â³ Network admin commands

### Chunk H8: Training Data Collection

- â³ Training data repository
- â³ Admin labeling interface
- â³ Automated collection
- â³ Export & training pipeline

## Repository Pattern Implementation

The repository pattern provides a clean abstraction for data access operations:

### BaseRepository Interface

```typescript
export interface BaseRepository<T> {
  findById(id: string): Promise<T | null>;
  findMany(filters?: any): Promise<T[]>;
  create(entity: Partial<T>): Promise<T>;
  update(id: string, entity: Partial<T>): Promise<T | null>;
  delete(id: string): Promise<boolean>;
}
```

### SupabaseRepository Implementation

```typescript
export class SupabaseRepository<T> implements BaseRepository<T> {
  constructor(
    protected supabase: SupabaseClient,
    protected tableName: string
  ) {}

  async findById(id: string): Promise<T | null> {
    try {
      const { data, error } = await this.supabase
        .from(this.tableName)
        .select('*')
        .eq('id', id)
        .single();

      if (error) {
        // Handle "not found" as a valid case
        if (error.code === 'PGRST116') {
          return null;
        }
        throw new RepositoryError(`Error finding ${this.tableName} by ID`, error);
      }

      return data as T;
    } catch (error) {
      this.handleError(error);
      return null;
    }
  }

  // Other methods implementation...
}
```

### Entity-Specific Repositories

```typescript
export class ServerRepository extends SupabaseRepository<ServerEntity> {
  constructor(supabase: SupabaseClient) {
    super(supabase, 'servers');
  }

  async findByGuildId(guildId: string): Promise<ServerEntity | null> {
    try {
      const { data, error } = await this.supabase
        .from(this.tableName)
        .select('*')
        .eq('guild_id', guildId)
        .single();

      if (error) {
        // Handle "not found" as a valid case
        if (error.code === 'PGRST116') {
          return null;
        }
        throw new RepositoryError(`Error finding server by guild ID`, error);
      }

      return data as ServerEntity;
    } catch (error) {
      this.handleError(error);
      return null;
    }
  }

  // Other server-specific methods...
}
```

## Migration Strategy

The transition from environment variables to database storage will follow these steps:

1. **Parallel Operation**:
   - Run database alongside environment variables
   - Read from database, fall back to environment variables
   - Log discrepancies

2. **Write Everywhere**:
   - Write to both database and memory
   - Validate consistency
   - Prefer database for reads

3. **Database Primary**:
   - Make database the primary source
   - Keep environment variables as fallback
   - Add migration warnings

4. **Full Migration**:
   - Remove environment variable fallbacks
   - Use database exclusively
   - Provide migration guide for users

## Testing Strategy

### Unit Testing Improvements

- âœ… Implement proper test isolation using unique IDs
- âœ… Add transaction-based rollbacks for database tests
- âœ… Create more focused, behavior-driven tests
- âœ… Improve mock implementation strategies
- âœ… Add proper cleanup hooks for all tests

### Integration Testing Enhancements

- â³ Set up proper test database environment
- ğŸ”„ Implement end-to-end workflow tests
- â³ Add performance testing for critical paths
- â³ Create realistic test data scenarios
- â³ Add proper test isolation strategies

### CI/CD Integration

- â³ Set up GitHub Actions for database tests
- â³ Configure test environment variables
- â³ Add migration verification steps
- â³ Implement proper test reporting
- â³ Add performance benchmarking

**Note**: GitHub Actions for database tests have not been implemented yet. This is planned for a future iteration.

1. **Unit Tests**:
   - Mock repositories for service tests
   - Test database operations in isolation
   - Validate edge cases

2. **Integration Tests**:
   - Use test database for integration testing
   - Test full workflows with persistence
   - Validate data consistency

3. **Migration Tests**:
   - Test fallback mechanism
   - Validate configuration migration
   - Test error recovery

## Access Patterns

The database schema is optimized for the following access patterns:

1. **Server Configuration Lookup**: Fast retrieval of server settings by guild_id
2. **User Reputation Check**: Quick access to a user's reputation both globally and per-server
3. **Detection History**: Efficient retrieval of past detection events for a user
4. **Analytics Aggregation**: Optimized for time-series reporting and dashboards
5. **Cross-Server Intelligence**: Designed to enable reputation sharing across trusted servers

## Data Retention Policies

Different data types have different retention requirements:

1. **Configuration Data**: Retained indefinitely (servers, settings)
2. **User Metadata**: Retained indefinitely, with optional anonymization after long periods of inactivity
3. **Detection Events**: Retained for 90 days by default, configurable per server
4. **Message Content**: Retained for 7 days by default, then purged or anonymized
5. **Analytics Data**: Aggregated and retained indefinitely, with raw data summarized after 30 days

## Backup Strategy

1. **Daily Point-in-Time Backups**: Automated daily snapshots
2. **Continuous Incremental Backups**: Transactional logs for point-in-time recovery
3. **Disaster Recovery**: Multi-region backup strategy for critical data
4. **Backup Testing**: Monthly restoration tests to verify integrity

## Security Considerations

1. **Row-Level Security**: Implemented for multi-tenant isolation
2. **API Access Control**: Restricted API access via Supabase auth
3. **Encryption**: Data encrypted at rest and in transit
4. **Audit Logging**: All significant database operations logged for security review

## Performance Optimizations

1. **Indexes**: Strategic indexes on frequently queried columns
2. **Caching**: Application-level caching for frequent lookups
3. **JSON Storage**: Flexible JSONB columns for extensibility without schema changes
4. **Partitioning**: Time-based partitioning for large tables (analytics, detection_events)

## Deployment Milestones

- ğŸ”„ Alpha Release (H1, H2)
  - ğŸ”„ Basic infrastructure
  - ğŸ”„ Core entity management
  - âœ… Initial test coverage
- ğŸ”„ Beta Release (H3, H4, H5)
  - ğŸ”„ Detection system
  - ğŸ”„ Context management
  - ğŸ”„ Basic analytics
- â³ Full Release (H6, H7, H8)
  - â³ Multi-server support
  - â³ Training data collection
  - â³ Advanced analytics
- â³ Post-Launch Review
  - â³ Performance analysis
  - â³ Test coverage review
  - â³ Documentation updates

## Future Considerations

1. **Scaling**:
   - Monitor database performance as user base grows
   - Implement additional caching as needed
   - Consider sharding for very large installations

2. **Analytics Dashboard**:
   - Create web interface for analytics
   - Add custom report generation
   - Implement visualizations

3. **Advanced Intelligence**:
   - Use collected data to train custom models
   - Implement more sophisticated detection algorithms
   - Add behavior analysis capabilities

4. **Privacy Controls**:
   - Implement granular data retention policies
   - Add user data anonymization
   - Create data export/deletion capabilities

5. **Environment-Specific Configurations**:
   - Development: Separate database with sample data, relaxed retention policies, debug-level logging
   - Production: Strict security policies, optimized performance, comprehensive backup strategy, limited access controls